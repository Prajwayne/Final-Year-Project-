{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmSfeOVPAyQQAwhLfPBryK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwayne/Final-Year-Project-/blob/main/Graph_Attention_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection in transaction datasets is an important problem, and GCNs are a powerful tool for identifying anomalies in graph-structured data. GCNs are a type of neural network that operate on graph data by propagating information between nodes through multiple layers of computation. This allows GCNs to capture the relational structure of the data, making them well-suited to tasks such as anomaly detection in transaction datasets.\n",
        "\n",
        "To build a GCN for anomaly detection in transaction datasets, you would need to first represent the transaction data as a graph. Each transaction would be represented as a node in the graph, and edges between nodes could represent various relationships between the transactions, such as temporal or spatial dependencies.\n",
        "\n",
        "Once you have represented the transaction data as a graph, you can use a GCN to detect anomalies. The GCN would take as input the graph representation of the transaction data, and use a series of graph convolutional layers to compute node-level representations of the transactions. These node-level representations can then be used to identify anomalous transactions that deviate from the normal patterns in the dataset.\n",
        "\n",
        "There are several variations of GCNs that could be used for anomaly detection in transaction datasets, such as Gated Graph Convolutional Networks (GGCNs) or Graph Attention Networks (GATs), which may be better suited to specific characteristics of the dataset. It is also important to properly preprocess the data and set appropriate hyperparameters to ensure optimal performance of the GCN.\n",
        "\n",
        "Overall, building a GCN for anomaly detection in transaction datasets can be a complex task, but it is a promising approach for identifying fraudulent transactions and can potentially improve the security and integrity of financial transactions."
      ],
      "metadata": {
        "id": "NBg96lvKKkqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Network Algorithm "
      ],
      "metadata": {
        "id": "XRIgztXmKs-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define a GATLayer class that implements a single GAT layer, which consists of a linear transformation of the input features followed by a self-attention mechanism. The attention weights are computed by taking the softmax of a linear transformation of the input features, and then applying them to the input features using a matrix multiplication. We also apply dropout and a LeakyReLU activation to the output of the layer.\n",
        "\n",
        "The GAT class then stacks multiple GAT layers to form a multi-layer GAT model. Finally, we define an output layer that maps the hidden representations to the output classes.\n",
        "\n",
        "To use this model for graph classification or node classification tasks, you would need to define an appropriate dataset and dataloader, and then train the model using an appropriate loss function and optimizer."
      ],
      "metadata": {
        "id": "ulRTHDllK1Zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvxaM55lKdmV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):\n",
        "        super(GATLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.alpha = alpha\n",
        "        self.attention = nn.Linear(in_features, out_features)\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.relu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        h = self.linear(x)\n",
        "        a = self.attention(x)\n",
        "\n",
        "        a = F.softmax(torch.matmul(a, a.transpose(1, 0)) / self.alpha, dim=1)\n",
        "        a = self.dropout(a)\n",
        "\n",
        "        h = torch.matmul(a, h)\n",
        "        h = self.relu(h)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, n_features, n_classes, n_hidden, n_layers, dropout, alpha):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            GATLayer(n_features, n_hidden, dropout, alpha)\n",
        "            if i == 0\n",
        "            else GATLayer(n_hidden, n_hidden, dropout, alpha)\n",
        "            for i in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.out_layer = nn.Linear(n_hidden, n_classes)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, adj)\n",
        "\n",
        "        x = self.out_layer(x)\n",
        "\n",
        "        return x\n"
      ]
    }
  ]
}